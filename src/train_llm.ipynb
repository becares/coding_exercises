{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Training a Large Language Model\n",
        "- **Objective**: Train a Large Language Model using the provided dataset. The LLM should be capable of generating PlantUML code for a given scenario (which is an input to the LLM).\n",
        "- **Platform**: The training can be conducted on Google Colab.\n",
        "- **Deliverable**: A trained LLM that can successfully generate PlantUML code from scenario descriptions. Please upload the weights of the LLM on HuggingFace after training the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVjjS0iUvn_v",
        "outputId": "bd7afc64-770f-410f-c195-64ef89e8ff0b"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets bitsandbytes accelerate peft # Comment it out when not using Colab\n",
        "USER_ACCESS_TOKEN = \"hf_...\" #Deleted private token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXjdxP4Ovdsj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel, PeftConfig\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we load the provided dataset in Huggingface [coai/plantuml_generation](https://huggingface.co/datasets/coai/plantuml_generation). The selected model to fine-tune is **Microsoft Phi 1.5**. Some preprocessing to the dataset is neccesary, making use of the tokenizer. As no evaluation split for the dataset is provided, this is left empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eX1LM-Wvdsn",
        "outputId": "9b964788-2314-4633-a998-fe86a26c91e2"
      },
      "outputs": [],
      "source": [
        "raw_dataset = load_dataset(\"coai/plantuml_generation\", \"default\", split=\"train\")#.select(range(16))\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", use_fast=True)\n",
        "\n",
        "# Add special character to the tokenizer, this also tells the model when to stop\n",
        "tokenizer.padding_side = 'right'\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(data):\n",
        "    return tokenizer(data[\"text\"], truncation=True)\n",
        "\n",
        "# Map the full dataset with the tokenizer\n",
        "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "eval_dataloader = None #No evaluation dataset provided"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the [QLoRA (Dettmers et al. 2023)](https://arxiv.org/abs/2305.14314) byte configuration. This is done in order to enable large models to be trained in consumer hardware. In this case, a 4Bit quantization is used. This configuration is adapted for modern GPUs, in my case, an RTX 3060 6GB is used. [Source](https://kaitchup.substack.com/p/phi-2-a-small-model-easy-to-fine) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIqnPKi2vdsp"
      },
      "outputs": [],
      "source": [
        "compute_dtype = getattr(torch, \"bfloat16\")\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                bnb_4bit_quant_type=\"nf4\",\n",
        "                                bnb_4bit_compute_dtype=compute_dtype,\n",
        "                                bnb_4bit_use_double_quant=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", \n",
        "                                             trust_remote_code=True, \n",
        "                                             quantization_config=bnb_config, \n",
        "                                             device_map=\"auto\",\n",
        "                                             torch_dtype=\"auto\")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        r=16,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules= [\"q_proj\",\"k_proj\",\"v_proj\",\"fc2\",\"fc1\"] #Network layers that will be affected\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, define the training hyperparameters and additional configuration to upload the model directly to Huggingface. Some of the hyperparameters have been obtained from the aforementioned source and [this one](https://medium.com/@amodwrites/a-definitive-guide-to-qlora-fine-tuning-falcon-7b-with-peft-78f500a1f337). We define the trainer and start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"finetuned_phi_15_plantuml_generation\",\n",
        "        push_to_hub=True,\n",
        "        hub_token=USER_ACCESS_TOKEN,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=12,\n",
        "        log_level=\"debug\",\n",
        "        save_steps=100,\n",
        "        logging_steps=25, \n",
        "        learning_rate=1e-4,\n",
        "        optim='paged_adamw_8bit',\n",
        "        bf16=True, #change to fp16 if you are using an older GPU\n",
        "        num_train_epochs=3,\n",
        "        warmup_steps=100,\n",
        "        lr_scheduler_type=\"linear\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    args=training_arguments,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can save again locally and in the hub the fine-tuned model after it finished. Additionally, the tokenizer is also uploaded. The model weights and tokenizer can be found under my [Huggingface page (becares)](https://huggingface.co/becares/finetuned_phi_15_plantuml_generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "11e92125a8c34fe79e9f4d0cd590c8c7",
            "443c00b9d18d48628f7c402e8f3651d4",
            "501b6c447cdc41059b75be105657ae64",
            "03f0dc1480134e9ebf61a4d74a63440e",
            "849acee289364ec3be519582b9dd0b4c",
            "a41e3d480b7741fc8bd7aca00bababa2",
            "44511208090c4fcfa6f05099db3948c8",
            "1f37490935fe49868b881cabb88e6165",
            "2b236bf1a0794c41ad9f9470a020929b",
            "79f8624988f04731abc8450a679fd951",
            "09185a444a834417943723fc1072403f"
          ]
        },
        "id": "oaO7aVQVKXZX",
        "outputId": "143c18f7-a6cf-462c-d378-c9d767416b2d"
      },
      "outputs": [],
      "source": [
        "save_directory = \"finetuned_phi_15_plantuml_generation\"\n",
        "model.save_pretrained(save_directory, push_to_hub=True, token=USER_ACCESS_TOKEN)\n",
        "tokenizer.save_pretrained(save_directory, push_to_hub=True, token=USER_ACCESS_TOKEN) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We may try the model directly giving it inputs. Make sure to run the imports and tokenizer cells in case you want to try the model without running the trainig."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPk0Veyxvdsp",
        "outputId": "d64e716a-f439-460b-910a-d7048f3a82ad"
      },
      "outputs": [],
      "source": [
        "config = PeftConfig.from_pretrained(\"becares/finetuned_phi_15_plantuml_generation\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\")\n",
        "model = PeftModel.from_pretrained(base_model, \"becares/finetuned_phi_15_plantuml_generation\")\n",
        "\n",
        "inputs = tokenizer(\"Generate a plantuml diagram...\", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03f0dc1480134e9ebf61a4d74a63440e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79f8624988f04731abc8450a679fd951",
            "placeholder": "​",
            "style": "IPY_MODEL_09185a444a834417943723fc1072403f",
            "value": " 1.04G/1.04G [00:47&lt;00:00, 24.3MB/s]"
          }
        },
        "09185a444a834417943723fc1072403f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11e92125a8c34fe79e9f4d0cd590c8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_443c00b9d18d48628f7c402e8f3651d4",
              "IPY_MODEL_501b6c447cdc41059b75be105657ae64",
              "IPY_MODEL_03f0dc1480134e9ebf61a4d74a63440e"
            ],
            "layout": "IPY_MODEL_849acee289364ec3be519582b9dd0b4c"
          }
        },
        "1f37490935fe49868b881cabb88e6165": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b236bf1a0794c41ad9f9470a020929b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "443c00b9d18d48628f7c402e8f3651d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a41e3d480b7741fc8bd7aca00bababa2",
            "placeholder": "​",
            "style": "IPY_MODEL_44511208090c4fcfa6f05099db3948c8",
            "value": "model.safetensors: 100%"
          }
        },
        "44511208090c4fcfa6f05099db3948c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "501b6c447cdc41059b75be105657ae64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f37490935fe49868b881cabb88e6165",
            "max": 1044073644,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b236bf1a0794c41ad9f9470a020929b",
            "value": 1044073644
          }
        },
        "79f8624988f04731abc8450a679fd951": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "849acee289364ec3be519582b9dd0b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a41e3d480b7741fc8bd7aca00bababa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
